{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dd471367-2d1d-45c0-9105-d7c9b422f23e",
    "deepnote_cell_height": 217.15625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Analyse tweets and articles on the 2021 Swiss CO2 law\n",
    "First, we import dependencies and variables used throughout the notebook. All following cells depend on the first two cells being run. The pandas 'set_options' are optional, but recommended for easier reading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "00001-3bedea5b-4984-4885-a6f2-83d659624247",
    "deepnote_cell_height": 516.4375,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     21.1875
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10627,
    "execution_start": 1651649335539,
    "source_hash": "cd018d6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk.data\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# pd.set_option('display.min_rows', 400)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', 100)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-8c93d972-d7ac-4db9-ab04-54df5649cc3f",
    "deepnote_cell_height": 368.96875,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     611,
     597.078125
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 177,
    "execution_start": 1651649389344,
    "owner_user_id": "baff208b-2bcb-44cc-a837-ff5f29b87963",
    "source_hash": "6ecf9b1"
   },
   "outputs": [],
   "source": [
    "# Import old Twitter data and exclude retweets.\n",
    "# tweets = pd.read_csv('Twitter/23.-30.4/Twitter_Week2.csv')\n",
    "# tweets = tweets[tweets['isRetweet'] == False]\n",
    "\n",
    "# Import new Twitter data (as of 3 May 2022)\n",
    "twitter_path = 'Downloaded_tweets/'\n",
    "twitter_files = [file for file in os.listdir(twitter_path) if '.txt' in file]\n",
    "tweets = pd.DataFrame()\n",
    "for file in twitter_files:\n",
    "    with open(twitter_path + file, 'r') as f:\n",
    "        weekly_tweets = f.read()\n",
    "    weekly_tweets = pd.DataFrame(json.loads(weekly_tweets)['data'])\n",
    "    tweets = pd.concat([tweets, weekly_tweets], ignore_index=True)\n",
    "\n",
    "tweets = tweets.drop(columns=['withheld'])\n",
    "\n",
    "# Use a sample of the dataset for tests. Comment the following line out to use the whole dataset.\n",
    "tweets = tweets.sample(10, random_state=42)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00003-21f546d7-d215-4830-a69d-2da418a0897c",
    "deepnote_cell_height": 638.96875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 651,
    "execution_start": 1651061099418,
    "source_hash": "26a28021"
   },
   "outputs": [],
   "source": [
    "splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "random.seed(42)\n",
    "\n",
    "# Import all German articles.\n",
    "de_path = 'Articles/Zeitungsartikel_DE/'\n",
    "de_files = [file for file in os.listdir(de_path) if os.path.isfile(os.path.join(de_path, file))]\n",
    "de_files = random.sample(de_files, 3) # Use a sample of the dataset for tests. Comment this line out to use the whole dataset.\n",
    "de_articles = pd.DataFrame(columns=['medium', 'date', 'day_index', 'author', 'text', 'language', 'characters_in_text'])\n",
    "for file in de_files:\n",
    "    row = []\n",
    "    row.append(file.split('_')[0]) # Add medium to row.\n",
    "    with open(de_path + file, 'r', encoding='cp1252', errors='ignore') as f:\n",
    "        article = f.read()\n",
    "    article = article.split('\\n') # Split the article when new line\n",
    "    split = article.pop(0).split(',') # Split first element of the article because it contains the medium and the date\n",
    "    row.append(split[1].replace(' ', '')) # Add date to row\n",
    "    if 'Art' in file:\n",
    "        row.append(int(file.split('.')[-2].split('_')[-1])) # Append character in file before the dot\n",
    "    else:\n",
    "        row.append(1) # If there is no 'Art' in file, then there is just one article for the day.\n",
    "    row.append('unknown')\n",
    "    for ele in article: # Search for string consisting of two words because it contains the author\n",
    "        if len(ele.split()) == 2:\n",
    "            row[3] = ele\n",
    "            break\n",
    "\n",
    "    text_length = len(' '.join(article))  # Get length of rejoined article\n",
    "    tokenized_text = splitter.tokenize(' '.join(article)) # Tokenize rejoined article which now only contains the text\n",
    "    row.append(tokenized_text) # Add tokenized text to row\n",
    "    row.append('de') # Add language to row\n",
    "    row.append(text_length) # Add length of rejoined article to row\n",
    "    de_articles.loc[len(de_articles)] = row # Append row to bottom of de_articles with iloc\n",
    "\n",
    "de_articles['date'] = pd.to_datetime(de_articles['date'], infer_datetime_format=True) # Transform date column to datetime format\n",
    "\n",
    "# Import all French articles.\n",
    "fr_path = 'Articles/Zeitungsartikel_FR/'\n",
    "fr_files = [file for file in os.listdir(fr_path) if os.path.isfile(os.path.join(fr_path, file))]\n",
    "fr_files = random.sample(fr_files, 3) # Use a sample of the dataset for tests. Comment this line out to use the whole dataset.\n",
    "fr_articles = pd.DataFrame(columns=['medium', 'date', 'day_index', 'author', 'text', 'language', 'characters_in_text'])\n",
    "for file in fr_files:\n",
    "    row = []\n",
    "    row.append(file.split('_')[0]) # Add medium to row.\n",
    "    with open(fr_path + file, 'r', encoding='cp1252', errors='ignore') as f:\n",
    "        article = f.read()\n",
    "    article = article.split('\\n') # Split the article when new line\n",
    "    split = article.pop(0).split(',') # Split first element of the article because it contains the medium and the date\n",
    "    row.append(split[1].replace(' ', '')) # Add date to row\n",
    "    if 'Art' in file:\n",
    "        row.append(int(file.split('.')[-2].split('_')[-1])) # Append character in file before the dot\n",
    "    else:\n",
    "        row.append(1) # If there is no 'Art' in file, then there is just one article for the day.\n",
    "    row.append('unknown')\n",
    "    for ele in article: # Search for string consisting of two words because it contains the author\n",
    "        if len(ele.split()) == 2:\n",
    "            row[3] = ele\n",
    "            break\n",
    "\n",
    "    text_length = len(' '.join(article))  # Get length of rejoined article\n",
    "    tokenized_text = splitter.tokenize(' '.join(article)) # Tokenize rejoined article which now only contains the text\n",
    "    row.append(tokenized_text) # Add tokenized text to row\n",
    "    row.append('fr') # Add language to row\n",
    "    row.append(text_length) # Add length of rejoined article to row\n",
    "    fr_articles.loc[len(fr_articles)] = row # Append row to bottom of fr_articles with iloc\n",
    "\n",
    "fr_articles['date'] = pd.to_datetime(fr_articles['date'], infer_datetime_format=True) # Transform date column to datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-3435ef17-f481-4bef-8269-6106d9382bca",
    "deepnote_cell_height": 109.96875,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Translate tweets and articles from German and French to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00005-29bbe460-0ba2-4278-8908-b9bce59abdeb",
    "deepnote_cell_height": 206.96875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14010,
    "execution_start": 1649705851915,
    "output_cleared": true,
    "source_hash": "a1b4fc59"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizers and the models.\n",
    "de_model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
    "de_tokenizer = AutoTokenizer.from_pretrained(de_model_name)\n",
    "de_model = AutoModelForSeq2SeqLM.from_pretrained(de_model_name)\n",
    "\n",
    "fr_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "fr_tokenizer = AutoTokenizer.from_pretrained(fr_model_name)\n",
    "fr_model = AutoModelForSeq2SeqLM.from_pretrained(fr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-60bc6dd6-da0b-4025-961b-949e12467e63",
    "deepnote_cell_height": 380.90625,
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     193.9375
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17595,
    "execution_start": 1649705924885,
    "source_hash": "32fc747b"
   },
   "outputs": [],
   "source": [
    "# Translate tweets.\n",
    "de_translation = pipeline(\"translation_de_to_en\", model=de_model, tokenizer=de_tokenizer)\n",
    "processed_text = [ele['translation_text'] for ele in de_translation(tweets.text.to_list())]\n",
    "tweets.insert(loc=2, column='processed_text', value=processed_text)\n",
    "tweets.to_csv('output/tweets_processed.csv')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00007-590613a4-73c3-4a2c-852a-afeef09fc616",
    "deepnote_cell_height": 353.96875,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# Translate articles and save English versions.\n",
    "\n",
    "# German articles.\n",
    "processed_texts = []\n",
    "for i, text in enumerate(de_articles.text.to_list()):\n",
    "    path = f\"Articles/Zeitungsartikel_EN/{de_articles.medium[i]}_{de_articles.date[i].strftime('%Y-%m-%d')}_{de_articles.day_index[i]}_en.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        batch = de_tokenizer(text, return_tensors='pt', padding=True) # Tokenize sentences.\n",
    "        output = de_model.generate(**batch, num_beams=2) # Generate English translations.\n",
    "        translation = ' '.join(de_tokenizer.batch_decode(output, skip_special_tokens=True)) # Decode translations.\n",
    "        processed_texts.append(translation)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(translation)\n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            translation = f.read()\n",
    "        processed_texts.append(translation)\n",
    "\n",
    "de_articles.insert(loc=5, column='processed_text', value=processed_texts)\n",
    "\n",
    "# French articles.\n",
    "processed_texts = []\n",
    "for i, text in enumerate(fr_articles.text.to_list()):\n",
    "    path = f\"Articles/Zeitungsartikel_EN/{fr_articles.medium[i]}_{fr_articles.date[i].strftime('%Y-%m-%d')}_{fr_articles.day_index[i]}_en.txt\"\n",
    "    if not os.path.exists(path):\n",
    "        batch = fr_tokenizer(text, return_tensors='pt', padding=True)\n",
    "        output = fr_model.generate(**batch, num_beams=2)\n",
    "        translation = ' '.join(fr_tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "        processed_texts.append(translation)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(translation)\n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            translation = f.read()\n",
    "        processed_texts.append(translation)\n",
    "\n",
    "fr_articles.insert(loc=5, column='processed_text', value=processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-7cb0b53e-5811-44d7-b541-1afb7f6694f1",
    "deepnote_cell_height": 197.953125,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Clean tweets for further processing\n",
    "Tweets are messy because they contain a lot of non-alphabetical symbols like URLs, hashtags and mentions. Hashtags are especially valuable for the analysis of the topic of the tweet because they tend to contain keywords. However, an algorithm wouldn't be able to automatically discern multiple keywords in a hashtag such as #supportCO2lawnow. Therefore, we need to exclude URLs and mentions, and separate hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_id": "00009-504f656f-6257-436f-b8f0-bae160f28f82",
    "deepnote_cell_height": 371.96875,
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from preprocessor import tokenize\n",
    "from re import findall, sub\n",
    "\n",
    "# Clean text, and find, split and replace hashtags.\n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "def clean(tweet):\n",
    "    hashtags = findall(r\"#(\\w+)\", tweet) # Find hashtags.\n",
    "    tweet_text = tokenize(tweet).split() # Split text into words.\n",
    "    tweet_text = ' '.join([word for word in tweet_text if '$' not in word or '$HASHTAG$' in word]) # Remove placeholders for URLs and mentions.\n",
    "    if hashtags:\n",
    "        hashtags = [seg_tw.segment(hashtag) for hashtag in hashtags] # Segment hashtags.\n",
    "        hashtags = [sub('\\s*([o])\\s*', r'\\1', hashtag) for hashtag in hashtags] # Remove spaces around 'o' to join 'co' and 2.\n",
    "        hashtags = [ele['translation_text'] for ele in de_translation(hashtags)] # Translate hashtags.\n",
    "        while hashtags:\n",
    "            tweet_text = tweet_text.replace('$HASHTAG$', hashtags.pop(0), 1) # Replace hashtags with segmented hashtags.\n",
    "    return tweet_text\n",
    "\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-b80f3f48-6f05-4717-868c-d84b867cfc68",
    "deepnote_cell_height": 170.765625,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Determine the tweets' and the articles' sentiment\n",
    "Ideally, the sentiment reflects whether a tweet is in support or against the CO2 law. Manual inspection has shown that this is indeed mostly the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00011-ad64b893-31c7-4070-8af7-328e6af4a1a5",
    "deepnote_cell_height": 229.640625,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5548,
    "execution_start": 1649705985193,
    "source_hash": "9ec5907"
   },
   "outputs": [],
   "source": [
    "# Infer sentiment from pretrained model.\n",
    "sentiment = pipeline('sentiment-analysis', \n",
    "                        model='distilbert-base-uncased-finetuned-sst-2-english', \n",
    "                        tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-3d64be7d-aca0-43f9-b014-0ace2dd36a18",
    "deepnote_cell_height": 689.9375,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 813,
    "execution_start": 1649705993658,
    "source_hash": "85e8b66e"
   },
   "outputs": [],
   "source": [
    "# Execute for tweets and save results.\n",
    "sentiment_output = sentiment(tweets.processed_text.to_list())\n",
    "tweets['sentiment_label'] = [ele['label'] for ele in sentiment_output]\n",
    "tweets['sentiment_score'] = [ele['score'] for ele in sentiment_output]\n",
    "tweets.to_csv('output/tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "00013-88257e5d-9d12-4323-89fd-3ebbdc213f6c",
    "deepnote_cell_height": 872.96875,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 29670,
    "execution_start": 1649685719641,
    "output_cleared": true,
    "source_hash": "7fbaf176"
   },
   "outputs": [],
   "source": [
    "# Combine German and French articles and save the dataset.\n",
    "articles = pd.concat([de_articles, fr_articles])\n",
    "\n",
    "# Execute sentiment analysis for articles.\n",
    "sentiment_label = []\n",
    "sentiment_score = []\n",
    "for text in articles.processed_text.to_list():\n",
    "    sentences = text.split('.') # Split article text into sentences.\n",
    "    sentiment_per_sentence = [sentiment(sentence)[0] for sentence in sentences] # Get sentiments for each sentence.\n",
    "    sentiment_score_per_sentence = [ele['score'] if ele['label'] == 'POSITIVE' else -ele['score'] for ele in sentiment_per_sentence] # Get sentiment scores.\n",
    "    mean_score = sum(sentiment_score_per_sentence) # Get mean sentiment score.\n",
    "    sentiment_label.append('POSITIVE' if mean_score > 0 else 'NEGATIVE') # Get sentiment label.\n",
    "    sentiment_score.append(mean_score)\n",
    "   \n",
    "articles['sentiment_label'] = sentiment_label\n",
    "articles['sentiment_score'] = sentiment_score\n",
    "\n",
    "articles.to_csv('output/articles_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-97003d78-d0c0-4b87-ba4c-fbdb7019eb9d",
    "deepnote_cell_height": 305.125,
    "deepnote_cell_type": "markdown",
    "owner_user_id": "e48bd0d0-7e07-4ff6-91c3-712c4adc010c"
   },
   "source": [
    "## Determine the tweets' topics and arguments in articles\n",
    "An unsupervised approach probably would not find the desired finegrained topics (or arguments) in the tweets. Therefore, we would have to manually label the tweets and subsequently train a supervised, finetuned model.\n",
    "Concerning the arguments in the articles, they could be entirely identified manually or with a finetuned Named Entity Recognition (NER) algorithm preceded by some manual labelling. The latter Machine Learning approach is significantly more complex but highly scalable.\n",
    "The open-source app [Label Studio](https://labelstud.io/) provides a powerful toolbox for human labelling. It is, however, necessary to develop a tailored interface, which would take a few days to complete. When completed, any person can use the app to label the tweets and articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=afd85eb4-e181-4004-a2b4-65d914f16510' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "0c7d8fde-4230-4625-b776-be6359d21c0f",
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
