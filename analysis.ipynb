{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Analyse tweets and articles on the 2021 Swiss CO2 law\nFirst, we import dependencies and variables used throughout the notebook. All following cells depend on the first two cells being run. The pandas 'set_options' are optional, but recommended for easier reading of the data.",
   "metadata": {
    "cell_id": "dd471367-2d1d-45c0-9105-d7c9b422f23e",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 194.78125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-3bedea5b-4984-4885-a6f2-83d659624247",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8818,
    "execution_start": 1651826244171,
    "source_hash": "f483d7bd",
    "owner_user_id": "baff208b-2bcb-44cc-a837-ff5f29b87963",
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369,
    "deepnote_output_heights": [
     null,
     21.1875
    ]
   },
   "source": "import pandas as pd\nimport os\nimport nltk.data\nimport random\nimport json\nimport re\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n\nnltk.download('punkt')\n\n# pd.set_option('display.min_rows', 400)\n# pd.set_option('display.max_rows', 400)\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_colwidth', 100)\n# pd.set_option('display.width', 1000)\n# pd.set_option('display.colheader_justify', 'center')\n# pd.set_option('display.precision', 3)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00002-8c93d972-d7ac-4db9-ab04-54df5649cc3f",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 156,
    "execution_start": 1651826340920,
    "source_hash": "6407ebfa",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 387
   },
   "source": "# Import old Twitter data and exclude retweets.\n# tweets = pd.read_csv('Twitter/23.-30.4/Twitter_Week2.csv')\n# tweets = tweets[tweets['isRetweet'] == False]\n\n# Import new Twitter data (as of 3 May 2022)\ntwitter_path = 'Tweets/'\ntwitter_files = [file for file in os.listdir(twitter_path) if '.txt' in file]\ntweets = pd.DataFrame()\nfor file in twitter_files:\n    with open(twitter_path + file, 'r') as f:\n        weekly_tweets = f.read()\n    weekly_tweets = pd.DataFrame(json.loads(weekly_tweets)['data'])\n    tweets = pd.concat([tweets, weekly_tweets], ignore_index=True)\n\ntweets = tweets.drop(columns=['withheld'])\n\n# Uncomment the following line to use a sample of the dataset for tests.\n# tweets = tweets.sample(3, random_state=42)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00003-21f546d7-d215-4830-a69d-2da418a0897c",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 940,
    "execution_start": 1651789634404,
    "source_hash": "87ff3696",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 801
   },
   "source": "def create_article_row(file, lang_path):\n    row = [file.split('_')[0]] # Instantiate row with the medium, e.g. 'NZZ'.\n    with open(lang_path + file, 'r', encoding='cp1252', errors='ignore') as f:\n        article = f.read()\n    article = article.split('\\n') # Split the article when new line\n    split = article.pop(0).split(',') # Split first line of the article because it contains the date.\n    row.append(split[1].replace(' ', ''))\n    if 'Art' in file:\n        row.append(int(file.split('.')[-2].split('_')[-1])) # Append character in file before the dot\n    else:\n        row.append(1) # If there is no 'Art' in file, then there is just one article for the day.\n    tokenized_text = splitter.tokenize(' '.join(article)) # Tokenize rejoined article which now only contains the text\n    row.append(tokenized_text) # Add tokenized text to row\n    row.append('de') # Add language to row\n    row.append(len(' '.join(article))) # Add length of rejoined article to row.\n\n    return row\n\nsplitter = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Import German articles.\nde_path = 'Articles/Zeitungsartikel_DE/'\nde_files = [file for file in os.listdir(de_path) if os.path.isfile(os.path.join(de_path, file))]\n# de_files = random.sample(de_files, 5) # Use a sample of the dataset for tests. Comment this line out to use the whole dataset.\nde_articles = pd.DataFrame(columns=['medium', 'date', 'day_index', 'text', 'language', 'characters_in_text'])\nfor file in de_files:\n    row = create_article_row(file, de_path)\n    de_articles.loc[len(de_articles)] = row # Append row to bottom of de_articles with iloc\n\nde_articles['date'] = pd.to_datetime(de_articles['date'], format='%d.%m.%Y') # Transform date column to datetime format\n\n# Import French articles.\nfr_path = 'Articles/Zeitungsartikel_FR/'\nfr_files = [file for file in os.listdir(fr_path) if os.path.isfile(os.path.join(fr_path, file))]\n# fr_files = random.sample(fr_files, 5) # Use a sample of the dataset for tests. Comment this line out to use the whole dataset.\nfr_articles = pd.DataFrame(columns=['medium', 'date', 'day_index', 'text', 'language', 'characters_in_text'])\nfor file in fr_files:\n    row = create_article_row(file, fr_path)\n    fr_articles.loc[len(fr_articles)] = row # Append row to bottom of fr_articles with iloc\n\nfr_articles['date'] = pd.to_datetime(fr_articles['date'], format='%d.%m.%Y') # Transform date column to datetime format",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Translate tweets and articles from German and French to English.",
   "metadata": {
    "cell_id": "00004-3435ef17-f481-4bef-8269-6106d9382bca",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 110
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00005-29bbe460-0ba2-4278-8908-b9bce59abdeb",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14650,
    "execution_start": 1651828006565,
    "output_cleared": true,
    "source_hash": "a1b4fc59",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 207
   },
   "source": "# Load the tokenizers and the models.\nde_model_name = 'Helsinki-NLP/opus-mt-de-en'\nde_tokenizer = AutoTokenizer.from_pretrained(de_model_name)\nde_model = AutoModelForSeq2SeqLM.from_pretrained(de_model_name)\n\nfr_model_name = 'Helsinki-NLP/opus-mt-fr-en'\nfr_tokenizer = AutoTokenizer.from_pretrained(fr_model_name)\nfr_model = AutoModelForSeq2SeqLM.from_pretrained(fr_model_name)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-60bc6dd6-da0b-4025-961b-949e12467e63",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10651896,
    "execution_start": 1651790798233,
    "source_hash": "24d88a17",
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "# Translate tweets.\nde_translation = pipeline(\"translation_de_to_en\", model=de_model, tokenizer=de_tokenizer)\nprocessed_text = [de_translation(tweet) for tweet in tweets.text.to_list()]\nprocessed_text = [ele[0]['translation_text'] for ele in processed_text]\ntweets.insert(loc=2, column='processed_text', value=processed_text)\ntweets.to_csv('output/tweets_processed.csv')\ntweets",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-590613a4-73c3-4a2c-852a-afeef09fc616",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9895e5df",
    "execution_start": 1651789635352,
    "execution_millis": 395,
    "output_cleared": false,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 801
   },
   "source": "# Translate articles and save English versions.\n\n# German articles.\ndef de_translate():\n    for i, text in enumerate(de_articles.text.to_list()):\n        path = f\"Articles/Zeitungsartikel_EN/{de_articles.medium[i]}_{de_articles.date[i].strftime('%Y-%m-%d')}_{de_articles.day_index[i]}_en.txt\"\n        if not os.path.exists(path):\n            for j, sentence in enumerate(text):\n                batch = de_tokenizer([sentence], return_tensors='pt', padding=True) # Tokenize sentence.\n                output = de_model.generate(**batch, num_beams=2) # Generate English translation.\n                text[j] = de_tokenizer.batch_decode(output, skip_special_tokens=True)[0] # Decode translations.\n            text = ' '.join(text)\n            with open(path, 'w') as f:\n                f.write(text)\n        else:\n            with open(path, 'r') as f:\n                text = f.read()\n        \n        yield text\n\nde_articles.insert(4, 'processed_text', [translation for translation in de_translate()])\n\n# French articles.\ndef fr_translate():\n    for i, text in enumerate(fr_articles.text.to_list()):\n        path = f\"Articles/Zeitungsartikel_EN/{fr_articles.medium[i]}_{fr_articles.date[i].strftime('%Y-%m-%d')}_{fr_articles.day_index[i]}_en.txt\"\n        if not os.path.exists(path):\n            for j, sentence in enumerate(text):\n                batch = fr_tokenizer([sentence], return_tensors='pt', padding=True)\n                output = fr_model.generate(**batch, num_beams=2)\n                text[j] = fr_tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n            text = ' '.join(text)\n            with open(path, 'w') as f:\n                f.write(text)\n        else:\n            with open(path, 'r') as f:\n                text = f.read()\n        \n        yield text\n\nfr_articles.insert(4, 'processed_text', [translation for translation in fr_translate()])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Clean tweets for further processing\nTweets are messy because they contain a lot of non-alphabetical symbols like URLs, hashtags and mentions. Hashtags are especially valuable for the analysis of the topic of the tweet because they tend to contain keywords. However, an algorithm wouldn't be able to automatically discern multiple keywords in a hashtag such as #supportCO2lawnow. Therefore, we need to exclude URLs and mentions, and separate hashtags.",
   "metadata": {
    "cell_id": "00008-7cb0b53e-5811-44d7-b541-1afb7f6694f1",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 175.578125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00009-504f656f-6257-436f-b8f0-bae160f28f82",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e0ef4001",
    "execution_start": 1651828025397,
    "execution_millis": 2781575,
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 405
   },
   "source": "from ekphrasis.classes.segmenter import Segmenter\nfrom preprocessor import tokenize\nfrom re import findall, sub\n\n# Clean text, and find, split and replace hashtags.\nseg_tw = Segmenter(corpus=\"twitter\")\ndef clean(tweet):\n    hashtags = findall(r\"#(\\w+)\", tweet) # Find hashtags.\n    tweet_text = tokenize(tweet).split() # Split text into words.\n    tweet_text = ' '.join([word for word in tweet_text if '$' not in word or '$HASHTAG$' in word]) # Remove placeholders for URLs and mentions.\n    if hashtags:\n        hashtags = [seg_tw.segment(hashtag) for hashtag in hashtags] # Segment hashtags.\n        hashtags = [sub('\\s*([o])\\s*', r'\\1', hashtag) for hashtag in hashtags] # Remove spaces around 'o' to join 'co' and 2.\n        hashtags = [ele['translation_text'] for ele in de_translation(hashtags)] # Translate hashtags.\n        while hashtags:\n            tweet_text = tweet_text.replace('$HASHTAG$', hashtags.pop(0), 1) # Replace hashtags with segmented hashtags.\n    return tweet_text\n\ntweets['processed_text'] = tweets['processed_text'].apply(clean)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Determine the tweets' and the articles' sentiment\nIdeally, the sentiment reflects whether a tweet is in support or against the CO2 law. Manual inspection has shown that this is indeed mostly the case.",
   "metadata": {
    "cell_id": "00010-b80f3f48-6f05-4717-868c-d84b867cfc68",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 130.78125
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-ad64b893-31c7-4070-8af7-328e6af4a1a5",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5278,
    "execution_start": 1651826427209,
    "source_hash": "9f7e1847",
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "# Instantiate a sentiment analysis pipeline with a pretrained model.\nsentiment = pipeline('sentiment-analysis', \n                        model='distilbert-base-uncased-finetuned-sst-2-english', \n                        tokenizer='distilbert-base-uncased',\n                        batch_size=1, \n                        device=-1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00012-3d64be7d-aca0-43f9-b014-0ace2dd36a18",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 251187,
    "execution_start": 1651830806972,
    "source_hash": "a7b9ff0d",
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "# Execute for tweets and save results.\nsentiment_output = [sentiment(tweet[:512])[0] for tweet in tweets.processed_text.to_list()]\ntweets['sentiment_label'] = [ele['label'] for ele in sentiment_output]\ntweets['sentiment_score'] = [ele['score'] for ele in sentiment_output]\ntweets.to_csv('output/tweets_processed.csv')\ntweets",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00013-88257e5d-9d12-4323-89fd-3ebbdc213f6c",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 615787,
    "execution_start": 1651788032097,
    "output_cleared": false,
    "source_hash": "d98c030b",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 387
   },
   "source": "# Combine German and French articles.\narticles = pd.concat([de_articles, fr_articles])\n\n# Execute sentiment analysis for articles.\nsentiment_label = []\nsentiment_score = []\nfor text in articles.processed_text.to_list():\n    sentences = text.split('.') # Split article text into sentences.\n    sentiment_per_sentence = [sentiment(sentence)[0] for sentence in sentences] # Get sentiments for each sentence.\n    sentiment_score_per_sentence = [ele['score'] if ele['label'] == 'POSITIVE' else -ele['score'] for ele in sentiment_per_sentence] # Get sentiment scores.\n    mean_score = sum(sentiment_score_per_sentence) / len(sentiment_score_per_sentence) # Get mean sentiment score.\n    sentiment_label.append('POSITIVE' if mean_score > 0 else 'NEGATIVE') # Get sentiment label.\n    sentiment_score.append(mean_score)\n   \narticles['sentiment_label'] = sentiment_label\narticles['sentiment_score'] = sentiment_score\n\narticles.to_csv('output/articles_processed.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Find the author name(s) of the articles\nThe author's or the authors' name(s) are hidden in the articles' texts. To find them we can use a Named Entity Recognition approach. It identifies names of people in text.",
   "metadata": {
    "cell_id": "3435eb2736fb486fb1b44218f278b97f",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 130.78125
   }
  },
  {
   "cell_type": "code",
   "source": "# del sentiment # Delete the model from the previous step to save memory.\narticles = pd.read_csv('output/articles_processed.csv')\n\nner_pipe = pipeline(\"ner\")\nfor row in articles.itertuples():\n    author = ''\n    for ele in ner_pipe(row.processed_text[:256]): # Search for names in the first 256 characters.\n        if ele['entity'] == 'I-PER':\n            author = f\"{author}{ele['word']}\"\n    author = author.replace('#', '') # Remove hashes.\n    author = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", author) # Insert a space before every capital letter.\n    articles.at[row.Index, 'author'] = author if author != '' else 'unknown'\n\narticles.to_csv('output/articles_processed.csv')",
   "metadata": {
    "cell_id": "4b86c59e41504ca0aec4fc80a6b6abf4",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f0eb064f",
    "execution_start": 1651789666993,
    "execution_millis": 138882,
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 315
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Review the results in the 'output' folder.",
   "metadata": {
    "cell_id": "0c19ef497a484d1bab3889295314f49a",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 70
   }
  },
  {
   "cell_type": "code",
   "source": "tweets = pd.read_csv('output/tweets_processed.csv')\ntweets",
   "metadata": {
    "cell_id": "ec509abcd1434192ac319229de234b8c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9248affc",
    "execution_start": 1651826385643,
    "execution_millis": 541,
    "deepnote_table_state": {
     "pageSize": 10,
     "pageIndex": 2,
     "filters": [],
     "sortBy": []
    },
    "deepnote_table_loading": false,
    "output_cleared": true,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "articles = pd.read_csv('output/articles_processed.csv')\narticles",
   "metadata": {
    "cell_id": "e7bd04358cfb461c80cf69475ee77a81",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 84
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Determine the tweets' topics and arguments in articles\nAn unsupervised approach probably would not find the desired finegrained topics (or arguments) in the tweets. Therefore, we would have to manually label the tweets and subsequently train a supervised, finetuned model.\nConcerning the arguments in the articles, they could be entirely identified manually or with a finetuned Named Entity Recognition (NER) algorithm preceded by some manual labelling. The latter Machine Learning approach is significantly more complex but highly scalable.\nThe open-source app [Label Studio](https://labelstud.io/) provides a powerful toolbox for human labelling. It is, however, necessary to develop a tailored interface, which would take a few days to complete. When completed, any person can use the app to label the tweets and articles.",
   "metadata": {
    "cell_id": "00014-97003d78-d0c0-4b87-ba4c-fbdb7019eb9d",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 220.375
   }
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=afd85eb4-e181-4004-a2b4-65d914f16510' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "0c7d8fde-4230-4625-b776-be6359d21c0f",
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 }
}