{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse tweets and articles on the 2021 Swiss CO2 law\n",
    "First, we import dependencies and variables used throughout the notebook. All following cells depend on the first two cells being run. The pandas 'set_options' are optional, but recommended for easier reading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk.data\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "nltk.download('punkt')\n",
    "\n",
    "# pd.set_option('display.min_rows', 400)\n",
    "# pd.set_option('display.max_rows', 400)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_colwidth', 100)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Twitter data and exclude retweets.\n",
    "tweets = pd.read_csv('Twitter/23.-30.4/Twitter_Week2.csv')\n",
    "tweets = tweets[tweets['isRetweet'] == False]\n",
    "\n",
    "# Use a sample of the dataset for tests. Comment the following line out to use the whole dataset.\n",
    "tweets = tweets.sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all German articles.\n",
    "de_path = 'Articles/Zeitungsartikel_DE/'\n",
    "de_files = [file for file in os.listdir(de_path) if os.path.isfile(os.path.join(de_path, file))]\n",
    "de_articles = []\n",
    "for file in de_files:\n",
    "    with open(de_path + file, 'r', errors='ignore') as f:\n",
    "        de_articles.append(f.read())\n",
    "\n",
    "# Import all French articles.\n",
    "fr_path = 'Articles/Zeitungsartikel_FR/'\n",
    "fr_files = [file for file in os.listdir(fr_path) if os.path.isfile(os.path.join(fr_path, file))]\n",
    "fr_articles = []\n",
    "for file in fr_files:\n",
    "    with open(fr_path + file, 'r', errors='ignore') as f:\n",
    "        fr_articles.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate tweets and articles from German and French to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizers and the models.\n",
    "de_model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
    "de_tokenizer = AutoTokenizer.from_pretrained(de_model_name)\n",
    "de_model = AutoModelForSeq2SeqLM.from_pretrained(de_model_name)\n",
    "\n",
    "fr_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
    "fr_tokenizer = AutoTokenizer.from_pretrained(fr_model_name)\n",
    "fr_model = AutoModelForSeq2SeqLM.from_pretrained(fr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate tweets.\n",
    "de_translation = pipeline(\"translation_de_to_en\", model=ger_model, tokenizer=ger_tokenizer)\n",
    "processed_text = [ele['translation_text'] for ele in de_translation(tweets.text.to_list())]\n",
    "tweets.insert(loc=1, column='processed_text', value=processed_text)\n",
    "tweets.to_csv('output/tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate articles and save English versions.\n",
    "splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# German articles.\n",
    "for i, article in enumerate(de_articles):\n",
    "    de_articles[i] = splitter.tokenize(article) # Split articles into sentences.\n",
    "    batch = de_tokenizer(de_articles[i], return_tensors='pt', padding=True) # Tokenize sentences.\n",
    "    output = de_model.generate(**batch) # Generate English translations.\n",
    "    translation = ' '.join(de_tokenizer.batch_decode(output, skip_special_tokens=True)) # Decode translations.\n",
    "    with open(f'Articles/Zeitungsartikel_uebersetzt/{de_files[i]}_en.txt', 'w') as f:\n",
    "        f.write(translation) # Save English translations.\n",
    "\n",
    "# French articles.\n",
    "for i, article in enumerate(fr_articles):\n",
    "    fr_articles[i] = splitter.tokenize(article)\n",
    "    batch = fr_tokenizer(fr_articles[i], return_tensors='pt', padding=True)\n",
    "    output = fr_model.generate(**batch)\n",
    "    translation = ' '.join(fr_tokenizer.batch_decode(output, skip_special_tokens=True))\n",
    "    with open(f'Articles/Zeitungsartikel_uebersetzt/{fr_files[i]}_en.txt', 'w') as f:\n",
    "        f.write(translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean tweets for further processing\n",
    "Tweets are messy because they contain a lot of non-alphabetical symbols like URLs, hashtags and mentions. Hashtags are especially valuable for the analysis of the topic of the tweet because they tend to contain keywords. However, an algorithm wouldn't be able to automatically discern multiple keywords in a hashtag such as #supportCO2lawnow. Therefore, we need mark URLs and mentions, and separate hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from preprocessor import tokenize\n",
    "from re import findall, sub\n",
    "\n",
    "# Clean text, and find, split and replace hashtags.\n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "def clean(tweet):\n",
    "    hashtags = findall(r\"#(\\w+)\", tweet) # Find hashtags.\n",
    "    tweet_text = tokenize(tweet).split() # Split text into words.\n",
    "    tweet_text = ' '.join([word for word in tweet_text if '$' not in word or '$HASHTAG$' in word]) # Remove placeholders for URLs and mentions.\n",
    "    if hashtags:\n",
    "        hashtags = [seg_tw.segment(hashtag) for hashtag in hashtags] # Segment hashtags.\n",
    "        hashtags = [sub('\\s*([o])\\s*', r'\\1', hashtag) for hashtag in hashtags] # Remove spaces around 'o' to join 'co' and 2.\n",
    "        while hashtags:\n",
    "            tweet_text = tweet_text.replace('$HASHTAG$', hashtags.pop(0), 1) # Replace hashtags with segmented hashtags.\n",
    "    return tweet_text\n",
    "\n",
    "tweets['processed_text'] = tweets['processed_text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the tweets' and the articles' sentiment\n",
    "Ideally, the sentiment reflects whether a tweet is in support or against the CO2 law. Manual inspection has shown that this is indeed mostly the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer sentiment from pretrained model.\n",
    "sentiment = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', tokenizer='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute for tweets and save results.\n",
    "sentiment_output = sentiment(tweets.processed_text.to_list())\n",
    "tweets['sentiment_label'] = [ele['label'] for ele in sentiment_output]\n",
    "tweets['sentiment_score'] = [ele['score'] for ele in sentiment_output]\n",
    "tweets.to_csv('output/tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute for German articles and save results.\n",
    "de_articles_sentiment = pd.DataFrame(de_files, columns=['file'])\n",
    "sentences_sample = []\n",
    "label = []\n",
    "score = []\n",
    "for article in de_articles:\n",
    "    sample = ' '.join(random.sample(article, 10)) # Sample 10 sentences from each article to not exceed the model's max length.\n",
    "    output = sentiment(sample) # Infer sentiment.\n",
    "    sentences_sample.append(sample) # Save sample sentences.\n",
    "    label.append(output[0]['label']) # Save sentiment labels.\n",
    "    score.append(output[0]['score']) # Save sentiment scores.\n",
    "\n",
    "de_articles_sentiment['sentences_sample'] = sentences_sample\n",
    "de_articles_sentiment['sentiment_label'] = label\n",
    "de_articles_sentiment['sentiment_score'] = score\n",
    "\n",
    "# Execute for French articles and save results.\n",
    "fr_articles_sentiment = pd.DataFrame(fr_files, columns=['file'])\n",
    "sentences_sample = []\n",
    "label = []\n",
    "score = []\n",
    "for article in fr_articles:\n",
    "    sample = ' '.join(random.sample(article, 10))\n",
    "    output = sentiment(sample)\n",
    "    sentences_sample.append(sample)\n",
    "    label.append(output[0]['label'])\n",
    "    score.append(output[0]['score'])\n",
    "\n",
    "fr_articles_sentiment['sentences_sample'] = sentences_sample\n",
    "fr_articles_sentiment['sentiment_label'] = label\n",
    "fr_articles_sentiment['sentiment_score'] = score\n",
    "\n",
    "# Merge both tables and save results\n",
    "articles_sentiment = pd.concat([de_articles_sentiment, fr_articles_sentiment], axis=0)\n",
    "articles_sentiment.to_csv('output/articles_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the tweets' topics and arguments in articles\n",
    "An unsupervised approach probably would not find the desired finegrained topics (or arguments) in the tweets. Therefore, we would have to manually label the tweets and subsequently train a supervised, finetuned model.\n",
    "Concerning the arguments in the articles, they could be entirely identified manually or with a finetuned Named Entity Recognition (NER) algorithm preceded by some manual labelling. The latter Machine Learning approach is significantly more complex but highly scalable.\n",
    "The open-source app [Label Studio](https://labelstud.io/) provides a powerful toolbox for human labelling. It is, however, necessary to develop a tailored interface, which would take a few days to complete. When completed, any person can use the app to label the tweets and articles."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
